{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "id": "6_3KOwW9R4OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from googletrans import Translator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add"
      ],
      "metadata": {
        "id": "W6vd6nazc9fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### datasource: kaggle.com/datasets/adityajn105/flickr8k"
      ],
      "metadata": {
        "id": "NJPdPNS_szMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up Google Translate client\n",
        "translator = Translator()"
      ],
      "metadata": {
        "id": "vD1qU25qai90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "captions_file = '/content/drive/MyDrive/captions.txt' \n",
        "images_folder = '/content/drive/MyDrive/cap_images'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKpidouOUv8v",
        "outputId": "577109aa-bf35-4bda-c4f7-9b8d1a9dc20a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load vgg16 model for extracting features\n",
        "model = VGG16()\n",
        "# remove last layer\n",
        "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)"
      ],
      "metadata": {
        "id": "AzfI3XERWe-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### image data"
      ],
      "metadata": {
        "id": "ZQYNb1KEph1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract features from image\n",
        "features = {}\n",
        "\n",
        "for img_id in os.listdir(images_folder):\n",
        "    # load the image from file\n",
        "    img_path = images_folder + '/' + img_id\n",
        "    image = load_img(img_path, target_size=(224, 224))\n",
        "    # convert image pixels to numpy array\n",
        "    image = img_to_array(image)\n",
        "    # reshape data for model\n",
        "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "    # preprocess image for vgg\n",
        "    image = preprocess_input(image)\n",
        "    # extract features\n",
        "    feature = model.predict(image, verbose=0)\n",
        "    # get image ID\n",
        "    image_id = img_id.split('.')[0]\n",
        "    # store feature\n",
        "    features[image_id] = feature"
      ],
      "metadata": {
        "id": "wMyKUWJFW0O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load features from the pickle file\n",
        "with open(os.path.join('/content/drive/MyDrive/features.pkl'), 'rb') as f:\n",
        "    features = pickle.load(f)"
      ],
      "metadata": {
        "id": "GMvWL04EXODH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### caption data"
      ],
      "metadata": {
        "id": "82Y7A058pcrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read captions from captions_file and create a dictionary from image_id to captions\n",
        "with open(os.path.join(captions_file), 'r') as f:\n",
        "    next(f)  # skip the header line\n",
        "    img_caption_dict = {}\n",
        "    for line in f:\n",
        "        # split the line by comma(,)\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "          image_id, caption = line.split(',', 1)\n",
        "          # remove .jpg\n",
        "          image_id = image_id.split('.')[0]\n",
        "          # convert caption list to string\n",
        "          caption = \" \".join(caption)\n",
        "          if image_id not in img_caption_dict:\n",
        "              img_caption_dict[image_id] = []\n",
        "          # store the caption\n",
        "          img_caption_dict[image_id].append(caption)"
      ],
      "metadata": {
        "id": "3GwyQ6uAXWvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(img_caption_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAo_6eOiXs-5",
        "outputId": "35c4dddf-bb91-4066-e7bd-2351a6d769bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8091"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_caption_dict['3767841911_6678052eb6']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpWVS4fmOUSj",
        "outputId": "58e26b33-0df7-4313-8e50-9295637abcd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['startseq baby girl in an orange dress gets wet as she stands next to water sprinkler endseq',\n",
              " 'startseq blonde toddler wearing an orange dress is wet and standing beside sprinkler in yard endseq',\n",
              " 'startseq child in dress is looking at sprinkler endseq',\n",
              " 'startseq little girl in an orange dress is running through the sprinkler in the yard endseq',\n",
              " 'startseq \"on wet grass little blond girl in orange dress plays in sprinkler .\" endseq']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_caption_dict['619169586_0a13ee7c21']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LB9vWu8jOqNI",
        "outputId": "af0ebf15-042f-49b3-bfa1-cd39da091367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['startseq woman in brown jacket is standing on rock with forested background endseq',\n",
              " 'startseq woman posing near cliff endseq',\n",
              " 'startseq woman standing in front of trees and smiling endseq',\n",
              " 'startseq woman stands on mountain overlooking rolling field of trees underneath blue sky endseq',\n",
              " 'startseq the person poses for picture on cliff overlooking valley endseq']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def caption_preprocessing(img_caption_dict):\n",
        "    for img_id, captions in img_caption_dict.items():\n",
        "        for i in range(len(captions)):\n",
        "            # take one caption at a time\n",
        "            caption = captions[i]\n",
        "            # lowercasing\n",
        "            caption = caption.lower()\n",
        "            # remove chars\n",
        "            caption = caption.replace('[^A-Za-z]', '')\n",
        "            # add start and end tags to the caption\n",
        "            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word)>1]) + ' endseq'\n",
        "            captions[i] = caption"
      ],
      "metadata": {
        "id": "0q9iuTykXwFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess the text\n",
        "caption_preprocessing(img_caption_dict)"
      ],
      "metadata": {
        "id": "hAULPmYaX3Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(img_caption_dict.items()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kPW3ETGX82n",
        "outputId": "14eabdc1-0970-467e-f64b-1f9915044088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1000268201_693b08cb0e',\n",
              " ['startseq child in pink dress is climbing up set of stairs in an entry way endseq',\n",
              "  'startseq girl going into wooden building endseq',\n",
              "  'startseq little girl climbing into wooden playhouse endseq',\n",
              "  'startseq little girl climbing the stairs to her playhouse endseq',\n",
              "  'startseq little girl in pink dress going into wooden cabin endseq'])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Captions = []\n",
        "for key in img_caption_dict:\n",
        "    for caption in img_caption_dict[key]:\n",
        "        Captions.append(caption)"
      ],
      "metadata": {
        "id": "FhrqaO_TYFD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(Captions) # all words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OLCipvKYFU-",
        "outputId": "7ed69bf9-b92a-4822-ecef-0863c1aab5ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40455"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(Captions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "vocab_size # unique words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9_2As6uYGra",
        "outputId": "7c171fdc-5167-470d-8b5c-8ef422382254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8485"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get caption maximum length\n",
        "max_sequence_length = max(len(caption.split()) for caption in Captions)\n",
        "max_sequence_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hplBWHc3YKIe",
        "outputId": "dd03d7c1-2d14-4435-edb9-c518c0413eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_ratio = 0.8\n",
        "image_ids = list(img_caption_dict.keys())\n",
        "split = int(len(image_ids) * split_ratio)\n",
        "training_data_ids = image_ids[:split]\n",
        "testing_data_ids = image_ids[split:]"
      ],
      "metadata": {
        "id": "r4XJPXXfYLrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create data generator to get data in batches (avoid session crash)\n",
        "def data_generator(img_ids, img_caption_dict, image_features, tokenizer, max_sequence_length, vocab_size, batch_size):\n",
        "    image_input = [] \n",
        "    sequence_input = [] \n",
        "    output = []\n",
        "    count = 0\n",
        "    while True:\n",
        "        for id in img_ids:\n",
        "            count += 1\n",
        "            captions = img_caption_dict[id]\n",
        "            # process each caption\n",
        "            for caption in captions:\n",
        "                # encode the sequence\n",
        "                sequence = tokenizer.texts_to_sequences([caption])[0]\n",
        "                # split the sequence into input and output pairs\n",
        "                for i in range(1, len(sequence)):\n",
        "                    # split into input and output pairs\n",
        "                    input_seq, output_seq = sequence[:i], sequence[i]\n",
        "                    # Pad the input sequence\n",
        "                    input_seq = pad_sequences([input_seq], maxlen=max_sequence_length)[0]\n",
        "                    # encode the output sequence\n",
        "                    output_seq = to_categorical([output_seq], num_classes=vocab_size)[0]\n",
        "                    # store the sequences\n",
        "                    image_input.append(image_features[id][0])\n",
        "                    sequence_input.append(input_seq)\n",
        "                    output.append(output_seq)\n",
        "            # yield the batch when the batch size is reached\n",
        "            if count == batch_size:\n",
        "                image_input = np.array(image_input)\n",
        "                sequence_input = np.array(sequence_input)\n",
        "                output = np.array(output)\n",
        "                yield [image_input, sequence_input], output\n",
        "                # reset\n",
        "                image_input = [] \n",
        "                sequence_input = [] \n",
        "                output = []\n",
        "                count = 0"
      ],
      "metadata": {
        "id": "o57Wo--BYPLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder model\n",
        "# image feature layers\n",
        "encoder_inputs = Input(shape=(4096,))\n",
        "encoder_fe1 = Dropout(0.5)(encoder_inputs)\n",
        "encoder_fe2 = Dense(256, activation='relu')(encoder_fe1)\n",
        "\n",
        "# input sequence layers\n",
        "decoder_inputs = Input(shape=(max_sequence_length,))\n",
        "decoder_se1 = Embedding(vocab_size, 256, mask_zero=True)(decoder_inputs)\n",
        "decoder_se2 = Dropout(0.5)(decoder_se1)\n",
        "decoder_se3 = LSTM(256)(decoder_se2)\n",
        "\n",
        "# decoder & encoder\n",
        "decoder_merged = add([encoder_fe2, decoder_se3])\n",
        "decoder_dense1 = Dense(256, activation='relu')(decoder_merged)\n",
        "decoder_outputs = Dense(vocab_size, activation='softmax')(decoder_dense1)\n",
        "\n",
        "\n",
        "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "eGDAZMj_YP0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training_data_ids the model\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "steps = len(training_data_ids) // batch_size\n",
        "\n",
        "for i in range(epochs):\n",
        "    # create data generator\n",
        "    generator = data_generator(training_data_ids, img_caption_dict, features, tokenizer, max_sequence_length, vocab_size, batch_size)\n",
        "    # fit for one epoch\n",
        "    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GensovBmYSQ9",
        "outputId": "907f76ae-d146-4c61-85ea-208e7f19e128"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "227/227 [==============================] - 1280s 6s/step - loss: 5.2409\n",
            "227/227 [==============================] - 1217s 5s/step - loss: 4.0276\n",
            "227/227 [==============================] - 1205s 5s/step - loss: 3.5893\n",
            "227/227 [==============================] - 1211s 5s/step - loss: 3.3237\n",
            "227/227 [==============================] - 1201s 5s/step - loss: 3.1274\n",
            "227/227 [==============================] - 1191s 5s/step - loss: 2.9758\n",
            "227/227 [==============================] - 1218s 5s/step - loss: 2.8609\n",
            "227/227 [==============================] - 1200s 5s/step - loss: 2.7660\n",
            "227/227 [==============================] - 1201s 5s/step - loss: 2.6881\n",
            "227/227 [==============================] - 1302s 6s/step - loss: 2.6216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model\n",
        "model.save('/content/drive/MyDrive/best_model.h5')"
      ],
      "metadata": {
        "id": "AItXzV8VYVhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def idx_to_word(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None"
      ],
      "metadata": {
        "id": "2NBQhVxyCprU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate caption for an image\n",
        "def predict_caption(model, image, tokenizer, max_length):\n",
        "    # add start tag for generation process\n",
        "    caption = 'startseq'\n",
        "    # iterate over the max length of sequence\n",
        "    for i in range(max_length):\n",
        "        # encode input sequence\n",
        "        sequence = tokenizer.texts_to_sequences([caption])[0]\n",
        "        # pad the sequence\n",
        "        sequence = pad_sequences([sequence], max_length)\n",
        "        # predict next word\n",
        "        predicted_word = model.predict([image, sequence], verbose=0)\n",
        "        # get index with high probability\n",
        "        predicted_word = np.argmax(predicted_word)\n",
        "        # convert index to word\n",
        "        word = idx_to_word(predicted_word, tokenizer)\n",
        "        # stop if word not found\n",
        "        if word is None:\n",
        "            break\n",
        "        # append word as input for generating next word\n",
        "        caption += \" \" + word\n",
        "        # stop if we reach end tag\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "\n",
        "    return caption"
      ],
      "metadata": {
        "id": "yGaHJzTLBDc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_model = VGG16()\n",
        "vgg_model = Model(inputs=vgg_model.inputs, outputs=vgg_model.layers[-2].output)"
      ],
      "metadata": {
        "id": "YQNi9amDIR3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = 'yandere.jpg'\n",
        "# load image\n",
        "image = load_img(image_path, target_size=(224, 224))\n",
        "# convert image pixels to numpy array\n",
        "image = img_to_array(image)\n",
        "# reshape data for model\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "# preprocess image for vgg\n",
        "image = preprocess_input(image)\n",
        "# extract features\n",
        "feature = vgg_model.predict(image, verbose=0)\n",
        "# predict \n",
        "predicted_caption = predict_caption(model, feature, tokenizer, 35).replace('startseq', '').replace('endseq', '').strip()\n",
        "translated_caption = translator.translate(predicted_caption, src=\"en\", dest=\"ar\")\n",
        "translated_text = translated_caption.text\n",
        "translated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gGomqv6uC2nV",
        "outputId": "b8f5b66b-72eb-4253-9a1d-9fa6504679e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'رجل في القميص الوردي يقف في الشارع'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = 'nurse.jpg'\n",
        "# load image\n",
        "image = load_img(image_path, target_size=(224, 224))\n",
        "# convert image pixels to numpy array\n",
        "image = img_to_array(image)\n",
        "# reshape data for model\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "# preprocess image for vgg\n",
        "image = preprocess_input(image)\n",
        "# extract features\n",
        "feature = vgg_model.predict(image, verbose=0)\n",
        "# predict \n",
        "predicted_caption = predict_caption(model, feature, tokenizer, 35).replace('startseq', '').replace('endseq', '').strip()\n",
        "translated_caption = translator.translate(predicted_caption, src=\"en\", dest=\"ar\")\n",
        "translated_text = translated_caption.text\n",
        "translated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_M0MV6q-IwHm",
        "outputId": "6bbc7866-daeb-4480-a7a6-80114a4c27b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'امرأة في القميص الوردي تقف في الشارع'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = 'girl.jpg'\n",
        "# load image\n",
        "image = load_img(image_path, target_size=(224, 224))\n",
        "# convert image pixels to numpy array\n",
        "image = img_to_array(image)\n",
        "# reshape data for model\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "# preprocess image for vgg\n",
        "image = preprocess_input(image)\n",
        "# extract features\n",
        "feature = vgg_model.predict(image, verbose=0)\n",
        "# predict \n",
        "predicted_caption = predict_caption(model, feature, tokenizer, 35).replace('startseq', '').replace('endseq', '').strip()\n",
        "translated_caption = translator.translate(predicted_caption, src=\"en\", dest=\"ar\")\n",
        "translated_text = translated_caption.text\n",
        "translated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SwVzhcZIJYak",
        "outputId": "19c33115-735a-451a-9ca2-25fc1162b42c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'فتاة صغيرة في اللباس الوردي تلعب مع خرطوم'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = 'leon.jpg'\n",
        "# load image\n",
        "image = load_img(image_path, target_size=(224, 224))\n",
        "# convert image pixels to numpy array\n",
        "image = img_to_array(image)\n",
        "# reshape data for model\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "# preprocess image for vgg\n",
        "image = preprocess_input(image)\n",
        "# extract features\n",
        "feature = vgg_model.predict(image, verbose=0)\n",
        "# predict \n",
        "predicted_caption = predict_caption(model, feature, tokenizer, 35).replace('startseq', '').replace('endseq', '').strip()\n",
        "translated_caption = translator.translate(predicted_caption, src=\"en\", dest=\"ar\")\n",
        "translated_text = translated_caption.text\n",
        "translated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "o3MmnsSlKUav",
        "outputId": "66bc3822-cef1-456f-f9c5-b4c2ac98ff5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'رجل في قميص أزرق وسترة زرقاء يسير في الشارع'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = 'human.jpg'\n",
        "# load image\n",
        "image = load_img(image_path, target_size=(224, 224))\n",
        "# convert image pixels to numpy array\n",
        "image = img_to_array(image)\n",
        "# reshape data for model\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "# preprocess image for vgg\n",
        "image = preprocess_input(image)\n",
        "# extract features\n",
        "feature = vgg_model.predict(image, verbose=0)\n",
        "# predict \n",
        "predicted_caption = predict_caption(model, feature, tokenizer, 35).replace('startseq', '').replace('endseq', '').strip()\n",
        "translated_caption = translator.translate(predicted_caption, src=\"en\", dest=\"ar\")\n",
        "translated_text = translated_caption.text\n",
        "translated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RnUAdF6PLBxY",
        "outputId": "8f4b98f0-77f7-47ee-a18f-bbe8399faed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'رجل يرتدي قميصًا أسود وقميصًا أسود يتحدث'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = 'dragon.jpg'\n",
        "# load image\n",
        "image = load_img(image_path, target_size=(224, 224))\n",
        "# convert image pixels to numpy array\n",
        "image = img_to_array(image)\n",
        "# reshape data for model\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "# preprocess image for vgg\n",
        "image = preprocess_input(image)\n",
        "# extract features\n",
        "feature = vgg_model.predict(image, verbose=0)\n",
        "# predict \n",
        "predicted_caption = predict_caption(model, feature, tokenizer, 35).replace('startseq', '').replace('endseq', '').strip()\n",
        "translated_caption = translator.translate(predicted_caption, src=\"en\", dest=\"ar\")\n",
        "translated_text = translated_caption.text\n",
        "translated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "m6wZhc2jL1RE",
        "outputId": "be572765-9dc4-4793-8298-0037783b519f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'رجل يرتدي قميصًا أحمر معلقًا على الجدار'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = 'harrypoter.jpg'\n",
        "# load image\n",
        "image = load_img(image_path, target_size=(224, 224))\n",
        "# convert image pixels to numpy array\n",
        "image = img_to_array(image)\n",
        "# reshape data for model\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "# preprocess image for vgg\n",
        "image = preprocess_input(image)\n",
        "# extract features\n",
        "feature = vgg_model.predict(image, verbose=0)\n",
        "# predict \n",
        "predicted_caption = predict_caption(model, feature, tokenizer, 35).replace('startseq', '').replace('endseq', '').strip()\n",
        "translated_caption = translator.translate(predicted_caption, src=\"en\", dest=\"ar\")\n",
        "translated_text = translated_caption.text\n",
        "translated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "n088KH_BL3yP",
        "outputId": "12af6ec5-ca7a-4142-cbe9-5a853f34f010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'رجل في قميص أسود وقميص أسود يلعب مع رأسه في الهواء'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = 'skating.jpg'\n",
        "# load image\n",
        "image = load_img(image_path, target_size=(224, 224))\n",
        "# convert image pixels to numpy array\n",
        "image = img_to_array(image)\n",
        "# reshape data for model\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "# preprocess image for vgg\n",
        "image = preprocess_input(image)\n",
        "# extract features\n",
        "feature = vgg_model.predict(image, verbose=0)\n",
        "# predict \n",
        "predicted_caption = predict_caption(model, feature, tokenizer, 35).replace('startseq', '').replace('endseq', '').strip()\n",
        "translated_caption = translator.translate(predicted_caption, src=\"en\", dest=\"ar\")\n",
        "translated_text = translated_caption.text\n",
        "translated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oWMSj7YeMR7q",
        "outputId": "6fdc49ae-dbbf-4bcf-f4af-3eb3e82cd6bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'تم طرح الرجل في الجزء العلوي من القارب في الجزء العلوي من القارب'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = 'dog.jpg'\n",
        "# load image\n",
        "image = load_img(image_path, target_size=(224, 224))\n",
        "# convert image pixels to numpy array\n",
        "image = img_to_array(image)\n",
        "# reshape data for model\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "# preprocess image for vgg\n",
        "image = preprocess_input(image)\n",
        "# extract features\n",
        "feature = vgg_model.predict(image, verbose=0)\n",
        "# predict \n",
        "predicted_caption = predict_caption(model, feature, tokenizer, 35).replace('startseq', '').replace('endseq', '').strip()\n",
        "translated_caption = translator.translate(predicted_caption, src=\"en\", dest=\"ar\")\n",
        "translated_text = translated_caption.text\n",
        "translated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Dg0Cr-VjMjoQ",
        "outputId": "52d6dfc9-56fe-4f0d-e96b-53e17abf7b06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'يمتد الكلب على طول الشاطئ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = 'old.jpg'\n",
        "# load image\n",
        "image = load_img(image_path, target_size=(224, 224))\n",
        "# convert image pixels to numpy array\n",
        "image = img_to_array(image)\n",
        "# reshape data for model\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "# preprocess image for vgg\n",
        "image = preprocess_input(image)\n",
        "# extract features\n",
        "feature = vgg_model.predict(image, verbose=0)\n",
        "# predict \n",
        "predicted_caption = predict_caption(model, feature, tokenizer, 35).replace('startseq', '').replace('endseq', '').strip()\n",
        "translated_caption = translator.translate(predicted_caption, src=\"en\", dest=\"ar\")\n",
        "translated_text = translated_caption.text\n",
        "translated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SBrqHG-2M4dH",
        "outputId": "aa2e7c3e-4c19-4b43-e9ee-3b4e23bc4d6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'رجل وامرأة يسيران على روكي هيلسايد'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = 'man.jpg'\n",
        "# load image\n",
        "image = load_img(image_path, target_size=(224, 224))\n",
        "# convert image pixels to numpy array\n",
        "image = img_to_array(image)\n",
        "# reshape data for model\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "# preprocess image for vgg\n",
        "image = preprocess_input(image)\n",
        "# extract features\n",
        "feature = vgg_model.predict(image, verbose=0)\n",
        "# predict \n",
        "predicted_caption = predict_caption(model, feature, tokenizer, 35).replace('startseq', '').replace('endseq', '').strip()\n",
        "translated_caption = translator.translate(predicted_caption, src=\"en\", dest=\"ar\")\n",
        "translated_text = translated_caption.text\n",
        "translated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "h4tM-nsJNI_b",
        "outputId": "92d4ef62-f968-4fd7-b595-4e4d44c2722b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'امرأة مع نظارات ونظارات تبتسم'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}